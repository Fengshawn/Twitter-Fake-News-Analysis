{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import json\n",
    "import math\n",
    "import os.path\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import preprocessing\n",
    "import gensim\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = gensim.models.KeyedVectors.load_word2vec_format('Set8_TweetDataWithoutSpam_GeneralData_Word_Phrase.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getExistence(source_tweets, filePath):\n",
    "#@@ Param:  Source_tweets: a list of source tweets ID\n",
    "#           filePath: source tweets  parent document directory\n",
    "#@@ Return: an n x 2 array, n is the amount/number of source tweets' IDs    \n",
    "    \n",
    "    existence = np.zeros((len(source_tweets),2))\n",
    "    for i in range(len(source_tweets)):\n",
    "        data = json.load(open(filePath+'/'+source_tweets[i]+'/source-tweet/'+source_tweets[i]+'.json','r'))\n",
    "        if len(data['entities']['hashtags']) != 0:\n",
    "            existence[i][0] = 1\n",
    "        else:\n",
    "            existence[i][0] = 0\n",
    "        \n",
    "        if len(data['entities']['urls']) != 0:\n",
    "            existence[i][1] = 1\n",
    "        else:\n",
    "            existence[i][1] = 0\n",
    "            \n",
    "    return existence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getText(source_tweets, filePath):\n",
    "#@@ Param:  Source_tweets: a list of source tweets ID\n",
    "#           filePath: source tweets  parent document directory\n",
    "#@@ Return: tweets text coresspoding to source tweets' IDs   \n",
    "\n",
    "    tweet_text = []\n",
    "    for i in range(len(source_tweets)):\n",
    "        data = json.load(open(filePath+'/'+source_tweets[i]+'/source-tweet/'+source_tweets[i]+'.json','r'))\n",
    "        tweet_text += [data['text']]\n",
    "    \n",
    "    return tweet_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUserInfo(source_tweets,featureList,filePath):\n",
    "    \n",
    "    userFeatures = []\n",
    "    for i in range(len(source_tweets)):\n",
    "        data = json.load(open(filePath+'/'+source_tweets[i]+'/source-tweet/'+source_tweets[i]+'.json','r'))\n",
    "        userData = [data['user'][feature] for feature in featureList]\n",
    "        userData += [data['created_at']]\n",
    "        userFeatures += [userData]\n",
    "    \n",
    "    return userFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features, Percentage of replying tweets classified as queries, denies or supports\n",
    "def getPercentage(source_tweets, tweets_stances, filePath):\n",
    "#@@ Param: source_tweets: a list of source tweets ID\n",
    "#          tweets_stancesï¼ša list of stances, which contains all tweets' stance including reply tweets and source tweets\n",
    "#          filePath: source tweets  parent document directory\n",
    "#@@ Return: an n x 3 array, n is the amount/number of source tweets' IDs\n",
    "   \n",
    "    stance_list = []\n",
    "    for tweet_ID in source_tweets:\n",
    "        replies_json_list = os.listdir(filePath+'/'+tweet_ID+'/replies') # make a list of reply tweets' IDs .json\n",
    "        replies_list = [dot_json.split('.')[0] for dot_json in replies_json_list] # remove filename suffixes '.json'\n",
    "        tmp = []\n",
    "        for reply_ID in replies_list:\n",
    "            tmp_stance = tweets_stances[reply_ID]\n",
    "            tmp += [tmp_stance]\n",
    "        stance_list += [tmp]\n",
    "    \n",
    "    \n",
    "    stance_percentage = np.zeros((len(stance_list),3)) # initialise an array, column 0,1,2 are percentages of query, deny, support\n",
    "    \n",
    "    for i in range(len(stance_list)):\n",
    "        count = Counter(stance_list[i])\n",
    "        l = len(stance_list[i])\n",
    "        if 'query' in count:\n",
    "            stance_percentage[i][0] = count['query']/l\n",
    "        else:\n",
    "            stance_percentage[i][0] = 0 \n",
    "        \n",
    "        if 'deny' in count:\n",
    "            stance_percentage[i][1] = count['deny']/l\n",
    "        else:\n",
    "            stance_percentage[i][1] = 0 \n",
    "        \n",
    "        if 'support' in count:\n",
    "            stance_percentage[i][2] = count['support']/l\n",
    "        else:\n",
    "            stance_percentage[i][2] = 0 \n",
    "    \n",
    "    return stance_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transfer label\n",
    "def str2no(y_string):\n",
    "#@@ Param: a list of rumour veracity labels, each element is a string\n",
    "#@@ Return: a list of label, each element is an int; 0,1,2 represent 'unverified','false','true' respectively\n",
    "    \n",
    "    y = []\n",
    "    for cls in y_string:\n",
    "        if cls=='unverified':\n",
    "            y += [0]\n",
    "        elif cls=='false':\n",
    "            y += [1]\n",
    "        elif cls=='true':\n",
    "            y += [2]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transferUserInfo(userInfo):\n",
    "    results = np.zeros((len(userInfo),len(userInfo[0])-1),dtype=int)\n",
    "    for i in range(len(userInfo)):\n",
    "        #if it has been verified\n",
    "        if userInfo[i][0]==False:\n",
    "            results[i][0] = 0\n",
    "        else:\n",
    "            results[i][0] = 1\n",
    "        \n",
    "        #if it has location?               \n",
    "        if userInfo[i][1]=='' or userInfo[i][1]==None:\n",
    "            results[i][1] = 0\n",
    "        else:\n",
    "            results[i][1] = 1\n",
    "            \n",
    "        #if it has description?\n",
    "        if userInfo[i][2]=='' or userInfo[i][2]==None:\n",
    "            results[i][2] = 0\n",
    "        else:\n",
    "            results[i][2] = 1\n",
    "\n",
    "        #how many followers?\n",
    "        results[i][3] = userInfo[i][3]               \n",
    "        #how many people it follows?\n",
    "        results[i][4] = userInfo[i][4]\n",
    "        #how many tweets it posted?\n",
    "        results[i][5] = userInfo[i][5]               \n",
    "        #how many days, after creating this account, when he/she posted this tweet\n",
    "        tp = time.strptime(userInfo[i][-1],\"%a %b %d %H:%M:%S %z %Y\")\n",
    "        tc = time.strptime(userInfo[i][-2],\"%a %b %d %H:%M:%S %z %Y\")\n",
    "        diff = (datetime.datetime(tp.tm_year, tp.tm_mon, tp.tm_mday) - datetime.datetime(tc.tm_year, tc.tm_mon, tc.tm_mday)).days\n",
    "        results[i][6] = diff\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scorer(y_truth, y_hat, confidence): # rumourEval 2019 version of scorer\n",
    "#@ Param:  y_truth: a list of true labels\n",
    "#          y_hat: a list of predicted y values\n",
    "#          confidence: a list of confidence values related to y_hat\n",
    "# Return: accuracy score, RMSE and Macro averaged F1 score\n",
    "    \n",
    "    correct = 0\n",
    "    total = len(y_hat)\n",
    "    errors = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for i in range(total):\n",
    "        if confidence[i]>0.5:\n",
    "                y_pred += [y_hat[i]]\n",
    "        else:\n",
    "                y_pred += [0]   \n",
    "        \n",
    "        if y_pred[i] == y_truth[i] and y_truth[i]!=0:\n",
    "            correct += 1\n",
    "            errors += [(1-confidence[i])**2]\n",
    "\n",
    "        elif y_truth[i] == 0:\n",
    "            errors += [ (confidence[i])**2 ]\n",
    "\n",
    "        else:\n",
    "            errors += [1.0]\n",
    "            \n",
    "    score = correct / total\n",
    "    rmse = math.sqrt( sum(errors) / len(errors) )\n",
    "    macroF = f1_score(y_truth, y_pred, average='macro')\n",
    "\n",
    "    return score,rmse,macroF,y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scorer(y_truth, y_hat, confidence): # rumourEval 2019 version of scorer\n",
    "#@ Param:  y_truth: a list of true labels\n",
    "#          y_hat: a list of predicted y values\n",
    "#          confidence: a list of confidence values related to y_hat\n",
    "# Return: accuracy score, RMSE and Macro averaged F1 score\n",
    "    \n",
    "    correct = 0\n",
    "    total = len(y_hat)\n",
    "    errors = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for i in range(total):\n",
    "        if confidence[i]>0.5:\n",
    "                y_pred += [y_hat[i]]\n",
    "        else:\n",
    "                y_pred += [0]   \n",
    "        \n",
    "        if y_pred[i] == y_truth[i] and y_truth[i]!=0:\n",
    "            correct += 1\n",
    "            errors += [(1-confidence[i])**2]\n",
    "\n",
    "        elif y_truth[i] == 0:\n",
    "            errors += [ (confidence[i])**2 ]\n",
    "\n",
    "        else:\n",
    "            errors += [1.0]\n",
    "            \n",
    "    score = correct / total\n",
    "    rmse = math.sqrt( sum(errors) / len(errors) )\n",
    "    macroF = f1_score(y_truth, y_pred, average='macro')\n",
    "\n",
    "    return score,rmse,macroF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the U,T,F of the source post\n",
    "train_file = './rumoureval-2019-training-data/train-key.json'\n",
    "f = json.load(open(train_file, 'r'))\n",
    "\n",
    "# extract features, hashtag existence and URL existence\n",
    "filePath = './rumoureval-2019-training-data/twitter-english'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_tweets_train = list(f['subtaskbenglish'].keys()) # make a list of source tweets' ID\n",
    "y_train_string = list(f['subtaskbenglish'].values())\n",
    "text = getText(source_tweets_train, filePath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile(filePath):\n",
    "    with open(filePath,'r') as f:\n",
    "        s = f.readlines()\n",
    "  \n",
    "    for i in range(len(s)):\n",
    "        s[i] = s[i].replace('\\n','')\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_string = readFile('./files/X_tr_string.txt')\n",
    "X_dev_string = readFile('./files/X_dev_string.txt')\n",
    "X_te_string = readFile('./files/X_te_string.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeText(x_raw):\n",
    "\n",
    "    x = x_raw.split(' ')\n",
    "    x.remove('')\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = [tokenizeText(x) for x in X_tr_string] ###############################\n",
    "X_dev = [tokenizeText(x) for x in X_dev_string] ################################ into token\n",
    "X_te = [tokenizeText(x) for x in X_te_string] ################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./files/dict.txt','r')\n",
    "f = f.read()\n",
    "word2id = eval(f) ###############################\n",
    "id2word = dict([val, key] for key, val in word2id.items()) ##############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toNumSeq(X_raw,vocab):\n",
    "  \n",
    "    doc = []\n",
    "  \n",
    "    for text in X_raw:\n",
    "        numSeq = []\n",
    "        for wrd in text:\n",
    "            if wrd in vocab:\n",
    "                numSeq += [vocab[wrd]]\n",
    "            else:\n",
    "                numSeq += [vocab['<OOV>']]\n",
    "        doc += [numSeq]\n",
    "  \n",
    "    return doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_num = toNumSeq(X_tr,word2id)############## num of sequence\n",
    "X_dev_num = toNumSeq(X_dev,word2id)############## num of sequence\n",
    "X_te_num = toNumSeq(X_te,word2id)############## num of sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word2id)+1,300))\n",
    "for word, i in word2id.items():\n",
    "    if word in emb:\n",
    "        embedding_vector = emb[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        embedding_matrix[i] = np.zeros(300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num2vec(X_num, embedding):\n",
    "  \n",
    "    vec_list = []\n",
    "    for x in X_num:\n",
    "        vec = []\n",
    "        for num in x:\n",
    "            vec += [embedding[num]]\n",
    "        vec_list += [vec]\n",
    "  \n",
    "    return vec_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_vec = num2vec(X_tr_num, embedding_matrix)\n",
    "X_dev_vec = num2vec(X_dev_num, embedding_matrix)\n",
    "X_te_vec = num2vec(X_te_num, embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgZeroVec(X_tmp):\n",
    "    \n",
    "    X_vec = X_tmp\n",
    "    # this is the code to turn all-zeros vectors in X_vec into average vectors \n",
    "    for i in range(len(X_vec)):\n",
    "        for j in range(len(X_vec[i])):\n",
    "            if np.all(X_vec[i][j]==0):\n",
    "                X_vec[i][j] = sum(X_vec[i])/len(X_vec[i])\n",
    "    return X_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_vec = avgZeroVec(X_tr_vec)\n",
    "X_dev_vec = avgZeroVec(X_dev_vec)\n",
    "X_te_vec = avgZeroVec(X_te_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRep(X_vec):\n",
    "    \n",
    "    X_rep = np.zeros((len(X_vec),300)) ####################################\n",
    "\n",
    "    for i in range(len(X_vec)): \n",
    "        #if X_vec[i]!=[]: # some tweets only contains mentions and url that were removed, so in X_vec, there are some [] array\n",
    "        X_rep[i] = sum(X_vec[i])/len(X_vec[i])\n",
    "    return X_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_rep = getRep(X_tr_vec)\n",
    "X_dev_rep = getRep(X_dev_vec)\n",
    "X_te_rep = getRep(X_te_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #extract train data\n",
    "\n",
    "# source_tweets_train = list(f['subtaskbenglish'].keys()) # make a list of source tweets' ID\n",
    "\n",
    "# # a dictionary of tweets' stances\n",
    "# tweets_stances_train = f['subtaskaenglish'] ################## train and dev data are using same datasets\n",
    "\n",
    "\n",
    "# features = ['verified', 'location', 'description', 'followers_count',  ####################user feature we need\n",
    "#                                   'friends_count', 'statuses_count', \"favourites_count\",'created_at']\n",
    "\n",
    "# userInfoString = getUserInfo(source_tweets_train,features,filePath)\n",
    "# #  hashtag existence, URL existence, percentage of queries, denies, supports\n",
    "# existence = getExistence(source_tweets_train, filePath)\n",
    "# qds_percentage = getPercentage(source_tweets_train, tweets_stances_train, filePath)\n",
    "# ue_train = transferUserInfo(userInfoString)\n",
    "\n",
    "# y_train_string = list(f['subtaskbenglish'].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing tweets' texts\n",
    "# train_text = getText(source_tweets_train, filePath)\n",
    "# train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = np.hstack((np.hstack((existence,qds_percentage)),ue_train)) ################# X_train : existence + qds_percent + userInfo\n",
    "y_train = np.array(str2no(y_train_string)) #################### y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################normalize the training data\n",
    "# scaler = preprocessing.StandardScaler().fit(X_train) \n",
    "# X_train = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#extract dev data\n",
    "\n",
    "dev_file = './rumoureval-2019-training-data/dev-key.json'\n",
    "f = json.load(open(dev_file, 'r'))\n",
    "\n",
    "source_tweets_dev = list(f['subtaskbenglish'].keys())\n",
    "y_dev_string = list(f['subtaskbenglish'].values())\n",
    "tweets_stances_dev = f['subtaskaenglish']\n",
    "\n",
    "# userInfoString = getUserInfo(source_tweets_dev,features,filePath)\n",
    "\n",
    "# existence = getExistence(source_tweets_dev, filePath)\n",
    "# qds_percentage = getPercentage(source_tweets_dev, tweets_stances_dev, filePath)\n",
    "# ue_dev = transferUserInfo(userInfoString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_dev = np.hstack((np.hstack((existence,qds_percentage)),ue_dev)) ################# X_dev\n",
    "# X_dev = scaler.transform(X_dev) ######################## normalize dev data\n",
    "y_dev = np.array(str2no(y_dev_string)) ################# y_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract test data\n",
    "\n",
    "te_Path = './rumoureval-2019-test-data/twitter-en-test-data'\n",
    "\n",
    "f = json.load(open('./final-eval-key.json', 'r'))\n",
    "\n",
    "source_tweets_te = list(f['subtaskbenglish'].keys())\n",
    "y_te_string = list(f['subtaskbenglish'].values())\n",
    "tweets_stances_te = f['subtaskaenglish']\n",
    "\n",
    "# userInfoString = getUserInfo(source_tweets_te,features,te_Path)\n",
    "\n",
    "# existence = getExistence(source_tweets_te, te_Path)\n",
    "# qds_percentage = getPercentage(source_tweets_te, tweets_stances_te, te_Path)\n",
    "# ue_test = transferUserInfo(userInfoString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_te = np.hstack((np.hstack((existence,qds_percentage)),ue_test)) ################# X_te\n",
    "# X_te = scaler.transform(X_te)####################normalize test data\n",
    "y_te = np.array(str2no(y_te_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_tr_rep\n",
    "X_dev = X_dev_rep\n",
    "X_te = X_te_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CalibratedClassifierCV(base_estimator=LinearSVC(C=10000, max_iter=100000),\n",
       "                       cv='prefit')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = svm.LinearSVC(multi_class='ovr', C=10000, max_iter=100000)\n",
    "clf.fit(X=X_train, y=y_train)\n",
    "sig_clf = CalibratedClassifierCV(clf,method='sigmoid', cv='prefit')\n",
    "sig_clf.fit(X_dev,y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_clf_probs = sig_clf.predict_proba(X_te) \n",
    "\n",
    "y_hat = sig_clf.predict(X_te) ######################### predicted label as 3 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 0, 1, 1, 2, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 2, 2, 2, 1, 1, 1, 1, 1,\n",
       "       1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 2, 2,\n",
       "       1, 2, 1, 1, 2, 1, 1, 2, 2, 1, 2, 2, 1, 2, 1, 1, 1, 0, 0, 1, 1, 1,\n",
       "       0, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_confidence = [sig_clf_probs[i][y_hat[i]] for i in range(len(y_hat))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.08928571428571429\n",
      "RMSE: 0.9386332318754439\n",
      "Macro averaged F1 socre: 0.07575757575757575\n"
     ]
    }
   ],
   "source": [
    "score,rmse,macroF,y_pred = scorer(y_te,y_hat,clf_confidence) # F-score is ill-defined and being set to 0.0 in labels with no predicted samples\n",
    "print('accuracy:', score)\n",
    "print('RMSE:', rmse)\n",
    "print('Macro averaged F1 socre:', macroF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  4  0]\n",
      " [25  5  0]\n",
      " [17  5  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         4\n",
      "           1       0.36      0.17      0.23        30\n",
      "           2       0.00      0.00      0.00        22\n",
      "\n",
      "    accuracy                           0.09        56\n",
      "   macro avg       0.12      0.06      0.08        56\n",
      "weighted avg       0.19      0.09      0.12        56\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shawn\\Anaconda3\\envs\\mlai19\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.figure\n",
    "print(confusion_matrix(y_te,y_pred))\n",
    "print(classification_report(y_te,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_confidence = np.zeros(y_hat.shape) ##################### confidence value for each piece of prediction\n",
    "\n",
    "for i in range(len(y_hat)):\n",
    "    if y_hat[i]!=0:\n",
    "        clf_confidence[i] = sig_clf_probs[i][y_hat[i]]\n",
    "    else:\n",
    "        if sig_clf_probs[i][1]>=sig_clf_probs[i][2]:\n",
    "            clf_confidence[i] = sig_clf_probs[i][1]\n",
    "            y_hat[i] = 1\n",
    "        else:\n",
    "            clf_confidence[i] = sig_clf_probs[i][2]\n",
    "            y_hat[i] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat ###################### after processing, y_hat should only contains 2 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 2, 2,\n",
       "       1, 2, 1, 1, 2, 1, 1, 2, 2, 1, 2, 2, 1, 2, 1, 1, 1, 0, 0, 1, 1, 1,\n",
       "       0, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scorer2017(y_truth, y_hat, confidence): # rumourEval 2019 version of scorer\n",
    "#@ Param:  y_truth: a list of true labels\n",
    "#          y_hat: a list of predicted y values\n",
    "#          confidence: a list of confidence values related to y_hat\n",
    "# Return: accuracy score, RMSE and Macro averaged F1 score\n",
    "    \n",
    "    correct = 0\n",
    "    total = len(y_hat)\n",
    "    errors = []\n",
    "    \n",
    "    for i in range(total):\n",
    "        \n",
    "        if y_hat[i] == y_truth[i] and y_truth[i]!=0:\n",
    "            correct += 1\n",
    "            errors += [(1-confidence[i])**2]\n",
    "\n",
    "        elif y_hat[i] == 0:\n",
    "            errors += [ (confidence[i])**2 ]\n",
    "\n",
    "        else:\n",
    "            errors += [1.0]\n",
    "    \n",
    "    score = correct / total\n",
    "    rmse = math.sqrt( sum(errors) / len(errors) )\n",
    "    macroF = f1_score(y_truth, y_hat, average='macro')\n",
    "\n",
    "    return score,rmse,macroF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scorer(y_truth, y_hat, confidence): # rumourEval 2019 version of scorer\n",
    "#@ Param:  y_truth: a list of true labels\n",
    "#          y_hat: a list of predicted y values\n",
    "#          confidence: a list of confidence values related to y_hat\n",
    "# Return: accuracy score, RMSE and Macro averaged F1 score\n",
    "    \n",
    "    correct = 0\n",
    "    total = len(y_hat)\n",
    "    errors = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for i in range(total):\n",
    "        if confidence[i]>0.5:\n",
    "                y_pred += [y_hat[i]]\n",
    "        else:\n",
    "                y_pred += [0]   \n",
    "        \n",
    "        if y_pred[i] == y_truth[i] and y_truth[i]!=0:\n",
    "            correct += 1\n",
    "            errors += [(1-confidence[i])**2]\n",
    "\n",
    "        elif y_truth[i] == 0:\n",
    "            errors += [ (confidence[i])**2 ]\n",
    "\n",
    "        else:\n",
    "            errors += [1.0]\n",
    "    \n",
    "    score = correct / total\n",
    "    rmse = math.sqrt( sum(errors) / len(errors) )\n",
    "    macroF = f1_score(y_truth, y_pred, average='macro')\n",
    "\n",
    "    return score,rmse,macroF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.08928571428571429\n",
      "RMSE: 0.9386332296775541\n",
      "Macro averaged F1 socre: 0.07575757575757575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shawn\\Anaconda3\\envs\\mlai19\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "score,rmse,macroF = scorer(y_te,y_hat,clf_confidence) # F-score is ill-defined and being set to 0.0 in labels with no predicted samples\n",
    "print('accuracy:', score)\n",
    "print('RMSE:', rmse)\n",
    "print('Macro averaged F1 socre:', macroF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "GaussianNB?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.19642857142857142\n",
      "RMSE: 0.8820189054487361\n",
      "Macro averaged F1 socre: 0.17675070028011206\n"
     ]
    }
   ],
   "source": [
    "clf = GaussianNB()\n",
    "clf.fit(X=X_train, y=y_train)\n",
    "sig_clf = CalibratedClassifierCV(clf,method='sigmoid', cv='prefit')\n",
    "sig_clf.fit(X_dev,y_dev)\n",
    "\n",
    "sig_clf_probs = sig_clf.predict_proba(X_te) \n",
    "\n",
    "y_hat = sig_clf.predict(X_te) ######################### predicted label as 3 classes\n",
    "\n",
    "clf_confidence = np.zeros(y_hat.shape) ##################### confidence value for each piece of prediction\n",
    "\n",
    "for i in range(len(y_hat)):\n",
    "    if y_hat[i]!=0:\n",
    "        clf_confidence[i] = sig_clf_probs[i][y_hat[i]]\n",
    "    else:\n",
    "        if sig_clf_probs[i][1]>=sig_clf_probs[i][2]:\n",
    "            clf_confidence[i] = sig_clf_probs[i][1]\n",
    "            y_hat[i] = 1\n",
    "        else:\n",
    "            clf_confidence[i] = sig_clf_probs[i][2]\n",
    "            y_hat[i] = 2\n",
    "\n",
    "score,rmse,macroF = scorer(y_te,y_hat,clf_confidence) # F-score is ill-defined and being set to 0.0 in labels with no predicted samples\n",
    "print('accuracy:', score)\n",
    "print('RMSE:', rmse)\n",
    "print('Macro averaged F1 socre:', macroF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.4107142857142857\n",
      "RMSE: 0.7868195052350737\n",
      "Macro averaged F1 socre: 0.21296296296296294\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=3200)\n",
    "clf.fit(X=X_train, y=y_train)\n",
    "sig_clf = CalibratedClassifierCV(clf,method='sigmoid', cv='prefit')\n",
    "sig_clf.fit(X_dev,y_dev)\n",
    "\n",
    "sig_clf_probs = sig_clf.predict_proba(X_te) \n",
    "\n",
    "y_hat = sig_clf.predict(X_te) ######################### predicted label as 3 classes\n",
    "\n",
    "clf_confidence = np.zeros(y_hat.shape) ##################### confidence value for each piece of prediction\n",
    "\n",
    "for i in range(len(y_hat)):\n",
    "    if y_hat[i]!=0:\n",
    "        clf_confidence[i] = sig_clf_probs[i][y_hat[i]]\n",
    "    else:\n",
    "        if sig_clf_probs[i][1]>=sig_clf_probs[i][2]:\n",
    "            clf_confidence[i] = sig_clf_probs[i][1]\n",
    "            y_hat[i] = 1\n",
    "        else:\n",
    "            clf_confidence[i] = sig_clf_probs[i][2]\n",
    "            y_hat[i] = 2\n",
    "\n",
    "score,rmse,macroF = scorer(y_te,y_hat,clf_confidence) # F-score is ill-defined and being set to 0.0 in labels with no predicted samples\n",
    "print('accuracy:', score)\n",
    "print('RMSE:', rmse)\n",
    "print('Macro averaged F1 socre:', macroF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.08928571428571429\n",
      "RMSE: 0.9386747663761195\n",
      "Macro averaged F1 socre: 0.07575757575757575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shawn\\Anaconda3\\envs\\mlai19\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "clf = svm.LinearSVC(multi_class='ovr', C=500, max_iter=100000)\n",
    "clf.fit(X=X_train, y=y_train)\n",
    "sig_clf = CalibratedClassifierCV(clf,method='sigmoid', cv='prefit')\n",
    "sig_clf.fit(X_dev,y_dev)\n",
    "\n",
    "sig_clf_probs = sig_clf.predict_proba(X_te) \n",
    "\n",
    "y_hat = sig_clf.predict(X_te) ######################### predicted label as 3 classes\n",
    "\n",
    "clf_confidence = np.zeros(y_hat.shape) ##################### confidence value for each piece of prediction\n",
    "\n",
    "for i in range(len(y_hat)):\n",
    "    if y_hat[i]!=0:\n",
    "        clf_confidence[i] = sig_clf_probs[i][y_hat[i]]\n",
    "    else:\n",
    "        if sig_clf_probs[i][1]>=sig_clf_probs[i][2]:\n",
    "            clf_confidence[i] = sig_clf_probs[i][1]\n",
    "            y_hat[i] = 1\n",
    "        else:\n",
    "            clf_confidence[i] = sig_clf_probs[i][2]\n",
    "            y_hat[i] = 2\n",
    "\n",
    "score,rmse,macroF = scorer(y_te,y_hat,clf_confidence) # F-score is ill-defined and being set to 0.0 in labels with no predicted samples\n",
    "print('accuracy:', score)\n",
    "print('RMSE:', rmse)\n",
    "print('Macro averaged F1 socre:', macroF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
